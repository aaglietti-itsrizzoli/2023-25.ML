services:
  sagemaker-training:
    image: my-custom-sagemaker-training-image
    build:
      context: .
      dockerfile: Dockerfile.train
    container_name: sagemaker-training
    command: ["tail", "-f", "/dev/null"]
    environment:
      - SAGEMAKER_PROGRAM=my-custom-training-script.py
      - SAGEMAKER_SUBMIT_DIRECTORY=/opt/ml/code
      - SM_INPUT_DIR=/opt/ml/input
      - SM_MODEL_DIR=/opt/ml/model
      - SM_OUTPUT_DATA_DIR=/opt/ml/output/data
    volumes:
      - ./src/train:/opt/ml/code
      - ./data/input:/opt/ml/input/data/training
      - ./data/output-compose/model:/opt/ml/model
      - ./data/output-compose/data:/opt/ml/output/data
    working_dir: /opt/ml/code
  sagemaker-inference:
    image: my-custom-sagemaker-inference-image
    build:
      context: .
      dockerfile: Dockerfile.inference
    container_name: sagemaker-inference
    # see https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-run-image
    command: ["serve"]
    environment:
        - AA_LOG_FILE=/opt/ml/output/data/logs-inference.txt
    volumes:
      - ./src/inference:/opt/ml/code
      - ./data/output/model:/opt/ml/model
      - ./data/output-compose/data:/opt/ml/output/data
    working_dir: /opt/ml/code
    ports:
        - 8080:8080
